{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQgVFUu201Vy"
   },
   "source": [
    "# IMPORT LIBRARY YANG DIPERLUKAN\n",
    "\n",
    "Pada tahap ini import library yang penting. untuk keperluan development model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4686,
     "status": "ok",
     "timestamp": 1730808069747,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "naoaba38YcdT"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import csv\n",
    "import dill\n",
    "import re\n",
    "\n",
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1xZYAhZYfB4"
   },
   "source": [
    "# SETUP CONSTANTS\n",
    "\n",
    "Dalam blok ini, kita mendefinisikan berbagai konstanta yang digunakan dalam proses pelatihan dan evaluasi model. Konstanta-konstanta ini meliputi path (lokasi direktori) untuk dataset, objek model, laporan pelatihan, serta hyperparameter penting seperti ukuran batch, jumlah epoch, ukuran gambar, dan parameter optimasi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJ6X0BWnYhCD"
   },
   "source": [
    "## function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1730808069747,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "kfX_2VKKYdwl"
   },
   "outputs": [],
   "source": [
    "def load_object(file_path):\n",
    "    \"\"\"\n",
    "    Fungsi untuk memuat objek dari file dengan menggunakan modul `dill`.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Lokasi file dari objek yang ingin dimuat.\n",
    "\n",
    "    Returns:\n",
    "        object: Objek yang dimuat dari file.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file_obj:\n",
    "            return dill.load(file_obj)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtIlj6hcYjWN"
   },
   "source": [
    "## main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730808069748,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "Rxn7aXl9YiIL"
   },
   "outputs": [],
   "source": [
    "# ==========================================================================================\n",
    "# ==================================== PARENT PATH CONSTANT ================================\n",
    "# ==========================================================================================\n",
    "PARENT_DATASET_PATH = \"..\"\n",
    "DATA_PATH = os.path.join(PARENT_DATASET_PATH, \"artifacts\", \"data\")\n",
    "OBJECT_PATH = os.path.join(PARENT_DATASET_PATH, \"artifacts\", \"objects\")\n",
    "REPORT_PATH = os.path.join(PARENT_DATASET_PATH, \"artifacts\", \"reports\")\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================================================\n",
    "# ==================================== DATA PATH CONSTANT ==================================\n",
    "# ==========================================================================================\n",
    "TRAIN_TFRECOARD_PATH = os.path.join(DATA_PATH, \"tfrecords\", \"train_trashnet.tfrecord\")\n",
    "VALID_TFRECORD_PATH = os.path.join(DATA_PATH, \"tfrecords\", \"valid_trashnet.tfrecord\")\n",
    "\n",
    "\n",
    "# ==========================================================================================\n",
    "# ==================================== REPORTS PATH CONSTANT ===============================\n",
    "# ==========================================================================================\n",
    "MODEL_NAME = \"VGG16\"\n",
    "TARGET_KERAS_MODEL_PATH = os.path.join(OBJECT_PATH, f\"{MODEL_NAME}_model.keras\")\n",
    "TABEL_TRAINING_PATH = os.path.join(REPORT_PATH, MODEL_NAME, \"tabel_pelatihan.csv\")\n",
    "TABEL_EPOCH_PATH = os.path.join(REPORT_PATH, MODEL_NAME, \"tabel_epoch.csv\")\n",
    "PLOT_TRAINING_PATH = os.path.join(REPORT_PATH, MODEL_NAME, \"plot_pelatihan.png\")\n",
    "PLOT_CONFUSION_MATRIX_PATH = os.path.join(REPORT_PATH, MODEL_NAME, \"plot_confusion_matrix.png\")\n",
    "CLASSIFICATION_REPORT_PATH = os.path.join(REPORT_PATH, MODEL_NAME, \"classification_report.txt\")\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================================================\n",
    "# =============================== OBJECT PATH CONSTANT  ====================================\n",
    "# ==========================================================================================\n",
    "LABEL_LIST_PATH = os.path.join(OBJECT_PATH, \"label_list.pkl\")\n",
    "CLASS_WEIGHTS_PATH = os.path.join(OBJECT_PATH, \"class_weights.pkl\")\n",
    "LABEL_LIST = load_object(LABEL_LIST_PATH)\n",
    "CLASS_WEIGHTS = load_object(CLASS_WEIGHTS_PATH)\n",
    "IMAGE_SIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKsVbyzlZaBI"
   },
   "source": [
    "# TRAINING WORKFLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHdA5ou2Z7IB"
   },
   "source": [
    "## Step 1. Build Architecture Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqcnbzGNZ88s"
   },
   "source": [
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "executionInfo": {
     "elapsed": 1181,
     "status": "ok",
     "timestamp": 1730808071361,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "zblCbqi6Z394",
    "outputId": "73dd680d-04a4-4c12-cacb-8a303547dadf"
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.VGG16 (\n",
    "    weights='imagenet',  # Menggunakan bobot dari pretrained model di ImageNet\n",
    "    include_top=False,   # Tidak menyertakan lapisan fully connected atas agar bisa menyesuaikan dataset kita\n",
    "    input_shape=(*IMAGE_SIZE, 3)  # Menyesuaikan input shape dengan ukuran gambar kita\n",
    ")\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_DYj3JyaArc"
   },
   "source": [
    "Pada langkah ini, kita akan membangun arsitektur model deep learning menggunakan **VGG16** sebagai base model (pretrained). Base model ini dilatih sebelumnya di dataset ImageNet dan akan digunakan sebagai extractor fitur untuk dataset kita.\n",
    "\n",
    "- **Base Model (VGG16)**: Arsitektur model ini digunakan sebagai dasar karena telah dilatih di dataset besar (ImageNet), sehingga dapat mengenali pola fitur visual umum dalam gambar. Kita tidak menggunakan lapisan output dari pretrained model ini (`include_top=False`) agar dapat menyesuaikan dengan dataset kita sendiri.\n",
    "- **Global Average Pooling**: Setelah melewati base model, kita menambahkan lapisan pooling untuk mereduksi dimensi output dari base model. Pooling ini mengambil rata-rata dari semua fitur untuk menghasilkan satu vektor per channel, sehingga dapat lebih mudah diproses di lapisan fully connected berikutnya.\n",
    "- **Output Layer**: Terakhir, kita menambahkan lapisan output dengan aktivasi sigmoid. Jika klasifikasi binari, lapisan ini menghasilkan 1 output node, sedangkan jika klasifikasi multi-kelas, lapisan ini menghasilkan output sebanyak jumlah kelas.\n",
    "- **Trainability**: Bagian pretrained model bisa dipilih apakah akan ikut dilatih atau tidak dengan mengatur `base_model_trainable`.\n",
    "\n",
    "Fungsi `build_model` digunakan untuk membangun dan meng-compile model dengan loss, metrics, optimizer yang didefinisikan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1730808071361,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "Y8cu0_BvZ6fN"
   },
   "outputs": [],
   "source": [
    "def build_model(\n",
    "      input_shape=None,\n",
    "      num_classes=None,\n",
    "      pretrained_model=None,\n",
    "    ):\n",
    "    # Input layer\n",
    "    input_layer = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Base model pretrained\n",
    "    pretrained_model.trainable = False\n",
    "\n",
    "    # Input melewati base model\n",
    "    x = pretrained_model(input_layer, training=False)\n",
    "\n",
    "    # Global average pooling layer\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Add fully connected tf.keras.layers with dropout\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "    # Output layer dengan aktivasi sigmoid\n",
    "    output_layer = tf.keras.layers.Dense(1 if len(num_classes) == 2 else len(num_classes), activation='softmax')(x)\n",
    "\n",
    "    # Membuat model\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oJiJgPt8LNj"
   },
   "source": [
    "Give `wandb.init` your config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 7417,
     "status": "ok",
     "timestamp": 1730808284662,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "YYSpEgj0aCwP",
    "outputId": "945c6f0a-b32f-4312-f4c5-5994e29260d2"
   },
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project='trashnet-adatama-test',\n",
    "    config={\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"loss_function\": \"sparse_categorical_crossentropy\",\n",
    "        \"metrics\": ['accuracy'],\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 150,\n",
    "        \"architecture\": \"VGG16\",\n",
    "        \"dataset\": \"TrashNet\",\n",
    "    }\n",
    ")\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "\n",
    "model = build_model(\n",
    "    input_shape=(*IMAGE_SIZE, 3),\n",
    "    num_classes=LABEL_LIST,\n",
    "    pretrained_model=base_model,\n",
    "  )\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=config.loss_function,\n",
    "    metrics=config.metrics\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsDFkrzTty0F"
   },
   "source": [
    "## Step 2. Load, Batching, Prefetch and Caching Dataset\n",
    "\n",
    "Pada tahap ini, kita menerapkan tiga teknik utama untuk meningkatkan performa pemrosesan data: batching, prefetching, dan caching.\n",
    "\n",
    "- **Load**: Memuat tensroflow dataset, untuk training model.\n",
    "- **Batching**: Membagi data menjadi grup kecil (batches) dengan ukuran tertentu (misalnya `BATCH_SIZE`). Ini bertujuan agar data bisa diproses lebih efisien selama pelatihan model.\n",
    "- **Prefetching**: Mengambil batch berikutnya dari dataset sebelum batch saat ini selesai diproses. Ini akan membantu dalam mengurangi waktu tunggu selama proses pelatihan model.\n",
    "- **Caching**: Dataset disimpan di memori (cache) setelah batch dan prefetch diterapkan. Ini mengurangi waktu baca dataset dari disk dan mempercepat proses akses data untuk batch selanjutnya.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCiTccEgt2At"
   },
   "source": [
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1730808080712,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "Xb7rLdAFsk_t"
   },
   "outputs": [],
   "source": [
    "def custom_title_print(title, n_strip=80):\n",
    "    \"\"\"\n",
    "    Prints a custom title with a surrounding strip of '=' characters.\n",
    "\n",
    "    Args:\n",
    "        title (str): The title to print.\n",
    "        n_strip (int, optional): Number of '=' characters used to format the title. Default is 80.\n",
    "    \"\"\"\n",
    "    print('=' * n_strip)\n",
    "    print(f' {title.upper()} '.center(n_strip, '='))\n",
    "    print('=' * n_strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730808081096,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "ItFOwFEwtK1x"
   },
   "outputs": [],
   "source": [
    "def show_data_info(**datasets):\n",
    "    \"\"\"\n",
    "    Menampilkan informasi detail tentang dataset yang diberikan.\n",
    "\n",
    "    Args:\n",
    "        **datasets: Satu atau lebih dataset yang ingin ditampilkan informasi.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        for dataset_name, dataset in datasets.items():\n",
    "            custom_title_print(f\"{dataset_name} info\")\n",
    "            print(f'info {dataset_name}: {dataset}')\n",
    "            print(f'number of {dataset_name}: {len(dataset)}')\n",
    "            print()\n",
    "    except Exception as e:\n",
    "        print(f'''\n",
    "            explicitly input parameter names such as:\n",
    "            show_data_info(train_dataset=train_ds, valid_dataset=valid_ds)\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730808081097,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "YTgocVCItM5L"
   },
   "outputs": [],
   "source": [
    "class DataInspector:\n",
    "    \"\"\"\n",
    "    Kelas `DataInspector` bertanggung jawab untuk melakukan inspeksi dan visualisasi\n",
    "    gambar dalam dataset pelatihan, validasi, dan pengujian.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, label_encoding, figsize):\n",
    "        \"\"\"\n",
    "        Inisialisasi kelas `DataInspector`.\n",
    "\n",
    "        Args:\n",
    "            label_encoding (dict): Mapping dari label numerik ke label kelas.\n",
    "            figsize (tuple): Ukuran figure untuk plot visualisasi gambar.\n",
    "        \"\"\"\n",
    "        self.label_encoding = label_encoding\n",
    "        self.figsize = figsize\n",
    "\n",
    "    def _custom_title_print(self, title, n_strip=80):\n",
    "        \"\"\"\n",
    "        Menampilkan judul yang diformat khusus dengan tanda pemisah.\n",
    "\n",
    "        Args:\n",
    "            title (str): Judul yang akan ditampilkan.\n",
    "            n_strip (int, optional): Jumlah karakter untuk garis pemisah. Default adalah 80.\n",
    "        \"\"\"\n",
    "        print('=' * n_strip)\n",
    "        print(f' {title.upper()} '.center(n_strip, '='))\n",
    "        print('=' * n_strip)\n",
    "\n",
    "    def _inspect_single_dataset(self, dataset, ds_name, ispath, idx):\n",
    "        \"\"\"\n",
    "        Helper function untuk menginspeksi dataset tertentu.\n",
    "\n",
    "        Args:\n",
    "            dataset (tf.data.Dataset): Dataset yang akan diinspeksi.\n",
    "            ds_name (str): Nama dataset (train, valid, test).\n",
    "            idx (int, optional): Indeks untuk memulai pengambilan contoh gambar. Default 1.\n",
    "        \"\"\"\n",
    "\n",
    "        plt.figure(figsize=self.figsize)\n",
    "\n",
    "        if ispath:\n",
    "            for i, (image, label, path) in enumerate(dataset.skip(idx).take(1), 1):\n",
    "                self._print_data_info(f\"{ds_name}_data info\", image, label, path)\n",
    "                self._plot_images(ds_name, image, label)\n",
    "        else:\n",
    "            for i, (image, label) in enumerate(dataset.skip(idx).take(1), 1):\n",
    "                self._print_data_info(f\"{ds_name}_data info\", image, label)\n",
    "                self._plot_images(ds_name, image, label)\n",
    "        plt.show()\n",
    "\n",
    "    def _print_data_info(self, title, image, label, image_path=None):\n",
    "        \"\"\"\n",
    "        Menampilkan informasi mendetail tentang gambar dan label.\n",
    "\n",
    "        Args:\n",
    "            title (str): Judul informasi yang akan ditampilkan.\n",
    "            image (tf.Tensor): Gambar yang diinspeksi.\n",
    "            label (int): Label gambar yang diinspeksi.\n",
    "            image_path (str, optional): Jalur file gambar (jika ada). Default adalah None.\n",
    "        \"\"\"\n",
    "        print('\\n\\n')\n",
    "        self._custom_title_print(title)\n",
    "\n",
    "        if image_path is not None:\n",
    "            print(f'image path: {image_path}')\n",
    "\n",
    "        print(f'shape-image: {image.shape}')\n",
    "        print(f'dtype-image: {image.dtype}')\n",
    "        print(f'max-intensity: {tf.reduce_max(image)}')\n",
    "        print(f'min-intensity: {tf.reduce_min(image)}')\n",
    "\n",
    "        print(f'label: {label} -> {self.label_encoding[label.numpy()]}')\n",
    "        print(f'label-shape: {label.shape}')\n",
    "        print(f'label-type: {label.dtype}')\n",
    "        print()\n",
    "\n",
    "    def _plot_images(self, ds_name, image, label):\n",
    "        \"\"\"\n",
    "        Memvisualisasikan gambar dari dataset pelatihan, validasi, dan pengujian secara dinamis.\n",
    "\n",
    "        Args:\n",
    "            datasets (dict): Dataset yang ingin di-plot (train_image, valid_image, test_image).\n",
    "        \"\"\"\n",
    "        plt.title(f'{ds_name.capitalize()} Label: {self.label_encoding[label.numpy()]}')\n",
    "        plt.imshow(image.numpy(), cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    def inspect(self, ispath=False, idx=1, **datasets):\n",
    "        \"\"\"\n",
    "        Menginspeksi gambar dari dataset pelatihan, validasi, atau pengujian (atau gabungan).\n",
    "\n",
    "        Args:\n",
    "            datasets (dict): Dataset yang ingin diinspeksi (train_ds, valid_ds, test_ds).\n",
    "                             Bisa masukkan satu atau lebih.\n",
    "        \"\"\"\n",
    "        # Looping dinamis sesuai dataset yang diberikan (train, valid, test)\n",
    "        for ds_name, ds in datasets.items():\n",
    "            self._inspect_single_dataset(ds, ds_name, ispath, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1730808081097,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "hJevIbwttwlE"
   },
   "outputs": [],
   "source": [
    "def display_info_dataset_batched(batch_size, dataset, dataset_batched, kind):\n",
    "    custom_title_print(f' {kind} ')\n",
    "    print(f\"Info data: {dataset_batched}\")\n",
    "    print(f\"Number of data: {len(dataset)}\")\n",
    "    if not re.search('test', kind.lower(), re.IGNORECASE):\n",
    "        print(f\"AFTER BATCH: {batch_size}\")\n",
    "        print(f\"Number of data: {len(dataset_batched)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WH-j827DtPMY"
   },
   "source": [
    "## main program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPKX4zzMtRWz"
   },
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730808082020,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "VMYH35hJtTcO"
   },
   "outputs": [],
   "source": [
    "train_tf_dataset_loaded = tf.data.Dataset.load(TRAIN_TFRECOARD_PATH, compression=\"GZIP\")\n",
    "valid_tf_dataset_loaded = tf.data.Dataset.load(VALID_TFRECORD_PATH, compression=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1730808082392,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "e7KIdSOBtU1E",
    "outputId": "f817b559-f003-434a-8344-a499f07f5727"
   },
   "outputs": [],
   "source": [
    "show_data_info(\n",
    "    train=train_tf_dataset_loaded,\n",
    "    valid=valid_tf_dataset_loaded\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1730808082393,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "9uDEmkdstWHD"
   },
   "outputs": [],
   "source": [
    "inspector = DataInspector(\n",
    "    LABEL_LIST,\n",
    "    figsize=(12,6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1949,
     "status": "ok",
     "timestamp": 1730808084803,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "VuG0asmgtXaU",
    "outputId": "11eb2564-3772-4dab-cf32-87831508ffed"
   },
   "outputs": [],
   "source": [
    "inspector.inspect(\n",
    "    train_dataset=train_tf_dataset_loaded,\n",
    "    valid_dataset=valid_tf_dataset_loaded,\n",
    "    idx=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvWtrt1CuEl0"
   },
   "source": [
    "Kita melakukan batch pada dataset training dan validasi, kemudian melakukan prefetching dan caching agar performa pelatihan lebih efisien. Dataset test hanya menggunakan prefetching dan caching, tanpa batching, karena umumnya tidak diperlukan batch untuk data test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1730808294243,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "6ZIYbFC_uCoR"
   },
   "outputs": [],
   "source": [
    "train_tf_images_batched = train_tf_dataset_loaded.batch(config.batch_size).prefetch(tf.data.AUTOTUNE).cache()\n",
    "valid_tf_images_batched = valid_tf_dataset_loaded.batch(config.batch_size).prefetch(tf.data.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1730808297804,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "nSYULN1JuG6c",
    "outputId": "cb15631f-3365-41a6-d89c-0a1b9c70c886"
   },
   "outputs": [],
   "source": [
    "display_info_dataset_batched(config.batch_size, train_tf_dataset_loaded, train_tf_images_batched, kind='train dataset')\n",
    "display_info_dataset_batched(config.batch_size, valid_tf_dataset_loaded, valid_tf_images_batched, kind='valid dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-modo_XaJMu"
   },
   "source": [
    "## Step 3. Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4636WV1baMVX"
   },
   "source": [
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1730808087399,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "q37KJEQCaHCj"
   },
   "outputs": [],
   "source": [
    "class TrainingLogger(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback untuk mencatat hasil pelatihan pada setiap epoch ke dalam file CSV.\n",
    "\n",
    "    Args:\n",
    "        log_file (str): Path ke file log CSV.\n",
    "        batch_size (int): Ukuran batch yang digunakan dalam pelatihan.\n",
    "    \"\"\"\n",
    "    def __init__(self, log_file, batch_size):\n",
    "        super(TrainingLogger, self).__init__()\n",
    "        self.log_file = log_file\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Membuat file CSV dan menulis header-nya jika belum ada\n",
    "        if not os.path.exists(log_file):\n",
    "            os.makedirs(os.path.dirname(self.log_file), exist_ok=True)\n",
    "            with open(log_file, mode='w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([\"Epoch\", \"Batch Size\", \"val_acc (%)\", \"val_loss (%)\"])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        Callback yang dipanggil pada akhir setiap epoch untuk mencatat\n",
    "        val_loss dan val_accuracy ke dalam file CSV.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Indeks epoch yang sedang berlangsung.\n",
    "            logs (dict): Dictionary yang menyimpan metrik pelatihan seperti val_loss dan val_accuracy.\n",
    "        \"\"\"\n",
    "        val_loss = logs.get('val_loss', 0) * 100  # Mengonversi ke persen\n",
    "        val_acc = logs.get('val_accuracy', 0) * 100  # Mengonversi ke persen\n",
    "\n",
    "\n",
    "        # Menulis hasil ke CSV\n",
    "        with open(self.log_file, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch + 1, self.batch_size, round(val_acc, 4), round(val_loss, 4)])\n",
    "\n",
    "\n",
    "# Callback untuk logging gambar prediksi\n",
    "class WandbImageLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data, label_list, sample_count=5):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.label_list = label_list\n",
    "        self.sample_count = sample_count\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Ambil batch pertama dari data validasi\n",
    "        images, labels = next(iter(self.validation_data))\n",
    "\n",
    "        # Ambil sejumlah sampel prediksi untuk logging\n",
    "        sample_images = images[:self.sample_count]\n",
    "        sample_labels = labels[:self.sample_count]\n",
    "        predictions = self.model.predict(sample_images)\n",
    "\n",
    "        wandb_images = []\n",
    "        for i in range(self.sample_count):\n",
    "            true_label = self.label_list[sample_labels[i].numpy()]\n",
    "            predicted_label = self.label_list[np.argmax(predictions[i])]\n",
    "\n",
    "            plt.figure()\n",
    "            plt.imshow(sample_images[i])\n",
    "            plt.title(f\"True: {true_label}, Pred: {predicted_label}\")\n",
    "\n",
    "            # Simpan gambar ke wandb\n",
    "            wandb_images.append(wandb.Image(plt, caption=f\"True: {true_label}, Pred: {predicted_label}\"))\n",
    "            plt.close()\n",
    "\n",
    "        # Log gambar-gambar ke wandb\n",
    "        wandb.log({\"predictions\": wandb_images}, step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1h9_BViaP_I"
   },
   "source": [
    "### main program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xpzpetGaSfY"
   },
   "source": [
    "#### setup callbacks\n",
    "\n",
    "Langkah ini mendefinisikan beberapa callback yang berguna selama proses pelatihan model, serta melakukan pelatihan model dengan callback yang ditentukan. Callback ini membantu memantau metrik dan menyimpan hasil model selama pelatihan.\n",
    "\n",
    "- **TrainingLogger**: Callback ini bertugas mencatat hasil pelatihan seperti `val_accuracy` dan `val_loss` setelah setiap epoch ke dalam file CSV. Hal ini membantu dalam memantau performa model di setiap epoch tanpa harus melihat output terminal secara langsung.\n",
    "- **ModelCheckpoint**: Callback ini menyimpan model setiap kali terjadi peningkatan performa berdasarkan nilai `val_loss`. Model terbaik akan disimpan di lokasi yang ditentukan dalam file `TARGET_KERAS_MODEL_PATH`.\n",
    "- **ReduceLROnPlateau**: Callback ini akan mengurangi learning rate secara otomatis jika model tidak menunjukkan peningkatan setelah beberapa epoch tertentu, berdasarkan parameter `patience`. Ini membantu agar model tidak terjebak di local minima dan bisa mencapai performa lebih baik.\n",
    "- **EarlyStopping**: Callback ini menghentikan pelatihan secara otomatis jika model tidak menunjukkan peningkatan setelah sejumlah epoch tertentu, dengan parameter `patience`. Hal ini bertujuan menghindari overfitting.\n",
    "\n",
    "Setelah semua callback didefinisikan, model dilatih menggunakan dataset yang sudah dibatch, divalidasi, dan dipantau oleh callback ini.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1730808303213,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "4rcHQmVzaPQq"
   },
   "outputs": [],
   "source": [
    "# Inisialisasi callback untuk logging hasil pelatihan\n",
    "training_logger = TrainingLogger(\n",
    "    log_file=TABEL_TRAINING_PATH,  # Path untuk menyimpan log hasil pelatihan\n",
    "    batch_size=config.batch_size  # Batch size yang digunakan\n",
    ")\n",
    "\n",
    "# Callback untuk menyimpan model terbaik berdasarkan val_loss terendah\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=TARGET_KERAS_MODEL_PATH,  # Path untuk menyimpan model terbaik\n",
    "    monitor='val_loss',  # Metode evaluasi adalah val_loss\n",
    "    save_best_only=True,  # Hanya simpan model terbaik\n",
    "    save_weights_only=False,  # Simpan seluruh model, bukan hanya bobot\n",
    "    mode='min',  # Simpan model ketika val_loss mencapai nilai terendah\n",
    "    verbose=1  # Tampilkan progress saat menyimpan model\n",
    ")\n",
    "\n",
    "# Callback untuk mengurangi learning rate jika val_loss tidak membaik setelah beberapa epoch\n",
    "plateau_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "  monitor='val_loss',  # Pantau val_loss\n",
    "  factor=0.5,  # Kurangi learning rate dengan faktor 0.5\n",
    "  patience=10,  # Berapa banyak epoch yang harus ditunggu sebelum menurunkan learning rate\n",
    "  verbose=1,  # Tampilkan informasi saat learning rate diturunkan\n",
    "  mode='auto',  # Secara otomatis mendeteksi apakah harus meminimalkan atau memaksimalkan\n",
    "  min_delta=0.001,  # Perubahan minimal pada val_loss yang diperlukan untuk dianggap sebagai peningkatan\n",
    "  cooldown=0,  # Berapa lama untuk menunggu sebelum menurunkan lagi setelah perbaikan\n",
    "  min_lr=0  # Learning rate minimal\n",
    ")\n",
    "\n",
    "# Callback untuk menghentikan pelatihan lebih awal jika val_loss tidak membaik\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "  monitor='val_loss',  # Pantau val_loss\n",
    "  patience=25,  # Berhenti jika tidak ada perbaikan setelah 25 epoch\n",
    "  restore_best_weights=True,  # Kembalikan bobot terbaik setelah selesai pelatihan\n",
    "  verbose=1  # Tampilkan progress penghentian pelatihan\n",
    ")\n",
    "\n",
    "# Tambahkan callback ini ke dalam model.fit()\n",
    "wandb_logger = WandbImageLogger(\n",
    "    validation_data=valid_tf_images_batched,  # Data validasi\n",
    "    label_list=LABEL_LIST,  # Daftar label (misalnya: ['cardboard', 'glass', ...])\n",
    "    sample_count=10  # Jumlah gambar yang ingin dilog setiap epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F46krovGaWeQ"
   },
   "source": [
    "#### training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 429,
     "status": "ok",
     "timestamp": 1730808308481,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "fAasYEdQ0jfl",
    "outputId": "055a1ec2-d759-4558-89bb-1046b8f17c74"
   },
   "outputs": [],
   "source": [
    "# Mapping string label names to numerical indices\n",
    "label_to_index = {label: idx for idx, label in enumerate(LABEL_LIST)}  # LABEL_LIST berisi daftar label secara urut\n",
    "\n",
    "# Convert CLASS_WEIGHTS to use integer keys\n",
    "class_weight_numeric = {label_to_index[label]: weight for label, weight in CLASS_WEIGHTS.items()}\n",
    "class_weight_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2903888,
     "status": "ok",
     "timestamp": 1730805143351,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "xZcdw9i8aVau",
    "outputId": "90e17024-28ed-4914-8579-12b6574921e8"
   },
   "outputs": [],
   "source": [
    "# Melatih model dengan dataset dan callback\n",
    "history = model.fit(\n",
    "    train_tf_images_batched,  # Dataset pelatihan yang sudah dibatch\n",
    "    validation_data=valid_tf_images_batched,  # Dataset validasi yang sudah dibatch\n",
    "    epochs=config.epochs,\n",
    "    class_weight=class_weight_numeric,\n",
    "    callbacks=[\n",
    "        training_logger,  # Logger untuk mencatat hasil pelatihan\n",
    "        checkpoint_callback,  # Simpan model terbaik\n",
    "        plateau_callback,  # Sesuaikan learning rate jika diperlukan\n",
    "        early_stopping, # Hentikan pelatihan jika tidak ada peningkatan\n",
    "        wandb_logger,\n",
    "        WandbMetricsLogger(),\n",
    "  ]\n",
    ")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ih14edBwObm"
   },
   "source": [
    "## Step 4. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1395329,
     "status": "ok",
     "timestamp": 1730809710042,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "6ODV_FreI_zf",
    "outputId": "d08e1c89-c88e-47bb-e2b7-0f3de17235c5"
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_classes, pretrained_model, dense_units=512, dropout_rate=0.5):\n",
    "    # Input layer\n",
    "    input_layer = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Freeze pretrained model\n",
    "    pretrained_model.trainable = False\n",
    "\n",
    "    # Passing input through pretrained base model\n",
    "    x = pretrained_model(input_layer, training=False)\n",
    "\n",
    "    # Global average pooling layer\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Fully connected layers with dropout\n",
    "    x = tf.keras.layers.Dense(dense_units, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    x = tf.keras.layers.Dense(dense_units // 4, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate / 2)(x)\n",
    "\n",
    "    # Output layer with softmax activation for multiclass classification\n",
    "    output_layer = tf.keras.layers.Dense(1 if len(num_classes) == 2 else len(num_classes), activation='softmax')(x)\n",
    "\n",
    "    # Compile the model\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "# Sweep Configuration\n",
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},  # Pastikan val_accuracy benar\n",
    "    'parameters': {\n",
    "        'learning_rate': {'values': [0.0001, 0.001]},\n",
    "        'batch_size': {'values': [16, 32, 64]},\n",
    "        'epochs': {'values': [15]},\n",
    "        'dropout_rate': {'values': [0.3, 0.5]},\n",
    "        'dense_units': {'values': [256, 512]}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize the sweep and run it\n",
    "sweep_id = wandb.sweep(sweep_config, project='trashnet-adatama-test')\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "      config = wandb.config\n",
    "\n",
    "      # Build and compile the model with sweep hyperparameters\n",
    "      model = build_model(\n",
    "          input_shape=(*IMAGE_SIZE, 3),\n",
    "          num_classes=LABEL_LIST,\n",
    "          pretrained_model=base_model,\n",
    "          dense_units=config.dense_units,\n",
    "          dropout_rate=config.dropout_rate\n",
    "\n",
    "      )\n",
    "\n",
    "      optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)\n",
    "      model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "      # Training and validation data\n",
    "      train_data = train_tf_images_batched\n",
    "      val_data = valid_tf_images_batched\n",
    "\n",
    "      # Train the model with the sweep configurations\n",
    "      model.fit(\n",
    "          train_tf_images_batched,\n",
    "          validation_data=valid_tf_images_batched,\n",
    "          epochs=config.epochs,\n",
    "          class_weight=class_weight_numeric,\n",
    "          callbacks=[\n",
    "              tf.keras.callbacks.LambdaCallback(\n",
    "                on_epoch_end=lambda epoch, logs: wandb.log({\"val_accuracy\": logs[\"val_accuracy\"], \"train_accuracy\": logs[\"accuracy\"]})\n",
    "            )\n",
    "          ]\n",
    "      )\n",
    "\n",
    "wandb.agent(sweep_id, function=train, count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2MU_UCIaqsB"
   },
   "source": [
    "# MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmi7MUS8asUZ"
   },
   "source": [
    "## function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1730805546140,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "bfOGjJrsarMz",
    "outputId": "e5b5a315-191d-4d8a-eb79-96bdfc6342d4"
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history, save_path):\n",
    "    \"\"\"\n",
    "    Plot training and validation accuracy and loss curves.\n",
    "\n",
    "    Args:\n",
    "    - history: History object yang dikembalikan oleh model.fit()\n",
    "    - save_path (str): Path di mana gambar kurva akan disimpan.\n",
    "    \"\"\"\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    # Plot akurasi\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Buat direktori jika belum ada, lalu simpan plot\n",
    "    dir_path = os.path.dirname(save_path)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1730805546498,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "kkIHLyiqat4J",
    "outputId": "a6166947-c18b-4d59-f1a1-2cbadb7e3e9d"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "      model,\n",
    "      tf_dataset,\n",
    "      class_names,\n",
    "      confusion_plot_path=None,\n",
    "      classification_report_path=None,\n",
    "      save_plot=True,\n",
    "      save_report=True,\n",
    "      normalize=False,  # Tambahin opsi untuk normalisasi confusion matrix\n",
    "      figsize=(6,4)\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Evaluasi model dan hasilkan confusion matrix serta classification report untuk klasifikasi biner atau multiclass.\n",
    "\n",
    "    Args:\n",
    "    - model: Model yang sudah dilatih.\n",
    "    - tf_dataset: Dataset untuk evaluasi.\n",
    "    - class_names: Daftar nama kelas yang ada.\n",
    "    - confusion_plot_path: Path untuk menyimpan confusion matrix plot.\n",
    "    - classification_report_path: Path untuk menyimpan classification report.\n",
    "    - save_plot: Simpan confusion matrix plot jika True.\n",
    "    - save_report: Simpan classification report jika True.\n",
    "    - normalize: Jika True, confusion matrix akan di-normalisasi (skala 0.0 - 1.0).\n",
    "    \"\"\"\n",
    "    # Ambil label asli dan prediksi model\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # Lakukan evaluasi pada setiap batch\n",
    "    for images, labels in tqdm(tf_dataset, desc='Evaluation'):\n",
    "        # Buat prediksi probabilitas untuk setiap gambar\n",
    "        predictions = model.predict(images, verbose=0)\n",
    "\n",
    "        # Ambil label asli\n",
    "        y_true.extend(tf.squeeze(labels).numpy())\n",
    "\n",
    "        # Cek apakah klasifikasi biner atau multiclass\n",
    "        if predictions.shape[1] == 1:\n",
    "            # Binary classification: ubah prediksi berdasarkan threshold 0.5\n",
    "            y_pred.extend((predictions > 0.5).astype(int))\n",
    "        else:\n",
    "            # Multiclass classification: ambil kelas dengan probabilitas tertinggi\n",
    "            y_pred.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Hitung confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Normalisasi confusion matrix jika diperlukan\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fmt = \".2f\"  # format untuk nilai desimal\n",
    "    else:\n",
    "        fmt = \"d\"  # format untuk nilai integer\n",
    "    title = 'Confusion Matrix (Jumlah)'\n",
    "\n",
    "    # Tampilkan dan simpan confusion matrix plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt=fmt, cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "\n",
    "    if save_plot:\n",
    "        plot_path = confusion_plot_path\n",
    "        plot_dir_path = os.path.dirname(plot_path)\n",
    "        os.makedirs(plot_dir_path, exist_ok=True)\n",
    "        plt.savefig(plot_path)\n",
    "        print(f\"Confusion matrix plot saved to {plot_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Cetak dan simpan classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    if save_report:\n",
    "        classification_report_dir_path = os.path.dirname(classification_report_path)\n",
    "        os.makedirs(classification_report_dir_path, exist_ok=True)\n",
    "        with open(classification_report_path, 'w') as f:\n",
    "            f.write(report)\n",
    "        print(f\"Classification report saved to {classification_report_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkHRfaw7axrJ"
   },
   "source": [
    "## main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 798
    },
    "executionInfo": {
     "elapsed": 1231,
     "status": "ok",
     "timestamp": 1730805548586,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "09566375417846211648"
     },
     "user_tz": -420
    },
    "id": "3MoVxO4_axA5",
    "outputId": "1edbc635-c433-455d-e7af-47ecdccd987e"
   },
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(valid_tf_images_batched)\n",
    "print(f'Model evaluation on validation data: {evaluation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pS6xQEXXa1Ri"
   },
   "source": [
    "### save tabel epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1730791096485,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "14716432972381704153"
     },
     "user_tz": -420
    },
    "id": "Vh4hk6hTa2PF",
    "outputId": "660ae65f-326d-4eda-a713-d782dfc4ccb3"
   },
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history).reset_index()\n",
    "history_df.rename(columns={'index': 'epoch'}, inplace=True)\n",
    "history_df['epoch'] = history_df['epoch'] + 1\n",
    "history_df = history_df.round({\n",
    "    'accuracy': 4,\n",
    "    'loss': 4,\n",
    "    'val_accuracy': 4,\n",
    "    'val_loss': 4,\n",
    "    'learning_rate': 10\n",
    "  }\n",
    ")\n",
    "\n",
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGRHTvmDa8Gk"
   },
   "outputs": [],
   "source": [
    "tabel_epoch_dir_path = os.path.dirname(TABEL_EPOCH_PATH)\n",
    "os.makedirs(tabel_epoch_dir_path, exist_ok=True)\n",
    "history_df.to_csv(TABEL_EPOCH_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xh4_WPhma_J3"
   },
   "source": [
    "### save plot training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "executionInfo": {
     "elapsed": 1611,
     "status": "ok",
     "timestamp": 1730791107357,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "14716432972381704153"
     },
     "user_tz": -420
    },
    "id": "6SD-0UwIa95s",
    "outputId": "43fb76de-a4e6-489a-b4f0-9b2b494ff518"
   },
   "outputs": [],
   "source": [
    "plot_training_history(history, PLOT_TRAINING_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnHM69FMbBwO"
   },
   "source": [
    "### save confusion matriks dan classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9IUSVHfbCZk"
   },
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model(TARGET_KERAS_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4102,
     "status": "ok",
     "timestamp": 1730791116973,
     "user": {
      "displayName": "Angga Dwi Sunarto",
      "userId": "14716432972381704153"
     },
     "user_tz": -420
    },
    "id": "Y-N89S8lbEhi",
    "outputId": "62d8e62f-7fd6-4651-9d02-0c9302f9c0b7"
   },
   "outputs": [],
   "source": [
    "evaluate_model(\n",
    "    best_model,\n",
    "    valid_tf_images_batched,\n",
    "    LABEL_LIST,\n",
    "    confusion_plot_path=PLOT_CONFUSION_MATRIX_PATH,\n",
    "    classification_report_path=CLASSIFICATION_REPORT_PATH,\n",
    "    save_plot=True,\n",
    "    save_report=True,\n",
    "    normalize=False,\n",
    "    figsize=(18, 12)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
